import os
import json
import re
import requests
from bs4 import BeautifulSoup
from database import get_chat_history
from ddgs import DDGS
from openai import OpenAI

# –∂—ë—Å—Ç–∫–æ –æ—Ç–∫–ª—é—á–∞–µ–º –ø—Ä–æ–∫—Å–∏ ENV (–Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π)
os.environ.pop("OPENAI_BASE_URL", None)
os.environ.pop("OPENAI_API_BASE", None)
os.environ.pop("OPENAI_ENDPOINT", None)

api_key = (os.getenv("OPENAI_API_KEY") or "").strip()
if not api_key:
    raise RuntimeError("OPENAI_API_KEY is not set")

client = OpenAI(api_key=api_key, base_url="https://api.openai.com/v1")

# ============================================================
#                     –í–ï–ë –ü–û–ò–°–ö (DDGS)
# ============================================================

def _call_search_api(search_query):
    try:
        with DDGS() as ddgs:
            results = list(
                ddgs.text(
                    search_query,
                    region="ru-ru",
                    safesearch="moderate",
                    max_results=5
                )
            )

        formatted_results = []
        for r in results:
            if r.get("href"):
                formatted_results.append({
                    "title": r.get("title", ""),
                    "snippet": r.get("body", ""),
                    "link": r.get("href", "")
                })

        return formatted_results

    except Exception as e:
        print(f"[FC][ERROR] web_search failed: {e}")
        return []




def _fetch_page_content(url):
    try:
        headers = {"User-Agent": "Mozilla/5.0"}
        html = requests.get(url, headers=headers, timeout=10).text
        soup = BeautifulSoup(html, "html.parser")
        text = " ".join(p.get_text() for p in soup.find_all("p"))
        return text[:4000]
    except:
        return ""


def _perform_web_search(query):
    cleaned = re.sub(
        r"^(–ø—Ä–∏–≤–µ—Ç|–∑–¥—Ä–∞–≤—Å—Ç–≤—É–π|–Ω–∞–π–¥–∏|—á—Ç–æ —Å–µ–π—á–∞—Å|–∫–∞–∫ –¥–µ–ª–∞)\s+",
        "",
        query,
        flags=re.I
    ).strip()

    search_query = f"{cleaned} lang:ru"
    results = _call_search_api(search_query)

    # üî• –í–û–¢ –ó–î–ï–°–¨ –õ–û–ì–ò–†–£–ï–ú
    log_web_search(search_query, results)

    if not results:
        return "üîç –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã."

    pages = []
    links = []

    for r in results[:10]:
        txt = _fetch_page_content(r["link"])
        if txt:
            pages.append(f"–ò—Å—Ç–æ—á–Ω–∏–∫: {r['title']} ({r['link']})\n{txt}\n")
            links.append(r)
            if len(pages) >= 3:
                break

    if not pages:
        return "üîç –ù–∞—à–ª–∏—Å—å —Å—Å—ã–ª–∫–∏, –Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –Ω–µ –∑–∞–≥—Ä—É–∑–∏–ª–∞—Å—å."

    final_text = "\n\n".join(pages)
    final_text += "\n\nüìö –ò—Å—Ç–æ—á–Ω–∏–∫–∏:\n" + "\n".join(
        [f"üîó [{r['title']}]({r['link']})" for r in links]
    )

    return final_text


import requests
from bs4 import BeautifulSoup

def fetch_url_content(url: str, max_chars: int = 12000) -> str:
    try:
        headers = {
            "User-Agent": "Mozilla/5.0"
        }
        r = requests.get(url, headers=headers, timeout=15)
        r.raise_for_status()

        soup = BeautifulSoup(r.text, "html.parser")

        # —É–¥–∞–ª—è–µ–º –º—É—Å–æ—Ä
        for tag in soup(["script", "style", "noscript"]):
            tag.decompose()

        text = soup.get_text(separator="\n")
        text = "\n".join(line.strip() for line in text.splitlines() if line.strip())

        return text[:max_chars]

    except Exception as e:
        return f"ERROR: –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É: {e}"

# ============================================================
#                       TOOLS
# ============================================================
tools = [
    {
        "type": "function",
        "function": {
            "name": "web_search",
            "description": "–ê–∫—Ç—É–∞–ª—å–Ω—ã–π –≤–µ–±-–ø–æ–∏—Å–∫ (DDGS)",
            "parameters": {
                "type": "object",
                "properties": {
                    "query": {"type": "string"}
                },
                "required": ["query"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "fetch_url",
            "description": "–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø–æ URL –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞",
            "parameters": {
                "type": "object",
                "properties": {
                    "url": {
                        "type": "string",
                        "description": "–ü–æ–ª–Ω—ã–π URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã"
                    }
                },
                "required": ["url"]
            }
        }
    }
]



# ============================================================
#                  Function-Calling Runner
# ============================================================

def run_fc(user_id: int, query: str, prompt: str, model="gpt-5.1-2025-11-13"):
    history = get_chat_history(user_id, limit=10)
    tools_used = []

    messages = [
        {"role": "system", "content": prompt},
        *history,
        {"role": "user", "content": query}
    ]


    print(f"[FC] User {user_id} | model={model}")
    print(f"[FC] –ó–∞–ø—Ä–æ—Å(120 —Å–∏–º–≤–æ–ª–æ–≤): {query[:120]!r}")

    # 1Ô∏è‚É£ –ü–µ—Ä–≤—ã–π –≤—ã–∑–æ–≤ ‚Äî –º–æ–¥–µ–ª—å —Ä–µ—à–∞–µ—Ç, –Ω—É–∂–µ–Ω –ª–∏ tool
    resp = client.chat.completions.create(
        model=model,
        messages=messages,
        tools=tools,
        tool_choice="auto"
    )

    msg = resp.choices[0].message
    tool_calls = getattr(msg, "tool_calls", None)

    # ‚ùå TOOLS –ù–ï –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–´
    if not tool_calls:
        print("[FC] ‚ö†Ô∏è tools NOT used")
        print("\n" + "‚îÄ" * 16 + " ASSISTANT PREVIEW " + "‚îÄ" * 16)
        print(msg.content[:300])
        print("‚îÄ" * 56 + "\n")

        return msg.content


    # ‚úÖ TOOLS –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–´
    print(f"[FC] Model decision: ‚úÖ tools USED ({len(tool_calls)})")

    messages.append(msg)

    # 2Ô∏è‚É£ –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ tool'–æ–≤
    for call in tool_calls:
        print(f"[FC] Tool called: {call.function.name}")
        if call.function.name == "fetch_url":
            args = json.loads(call.function.arguments)
            url = args.get("url")

            print(f"[FC] fetch_url: {url}")

            content = fetch_url_content(url)

            messages.append({
                "tool_call_id": call.id,
                "role": "tool",
                "name": "fetch_url",
                "content": content
            })


        if call.function.name == "web_search":
            tools_used = True

            args = json.loads(call.function.arguments)
            search_query = args.get("query", "")
            print(f"[FC] web_search query: {search_query!r}")

            result = _perform_web_search(search_query)

            if not result:
                print("[FC] web_search result: ‚ùå empty")
            else:
                print(f"[FC] web_search result length: {len(result)}")

            messages.append({
                "tool_call_id": call.id,
                "role": "tool",
                "name": "web_search",
                "content": result or ""
            })
        if tools_used:
            print("[FC] üîß tools USED:")
            for call in tool_calls:
                print(f" - {call.function.name}")

            tools_policy = (
                "–í–ê–ñ–ù–û:\n"
                "- –¢—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç web_search.\n"
                "- –¢–µ–±–µ –ó–ê–ü–†–ï–©–ï–ù–û –æ–±—ä—è—Å–Ω—è—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é, –∫–∞–∫ –∏—Å–∫–∞—Ç—å –≤—Ä—É—á–Ω—É—é.\n"
                "- –¢—ã –û–ë–Ø–ó–ê–ù –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã web_search –≤ –æ—Ç–≤–µ—Ç–µ.\n"
                "- –ï—Å–ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã ‚Äî –ø—Ä—è–º–æ —Å–∫–∞–∂–∏: "
                "'–ü–æ–∏—Å–∫ –¥–∞–ª –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã'.\n"
                "- –ù–µ –¥–∞–≤–∞–π –æ–±—â–∏—Ö —Å–æ–≤–µ—Ç–æ–≤ –∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –±–µ–∑ —Å—Å—ã–ª–æ–∫ –∏–∑ –ø–æ–∏—Å–∫–∞.\n"
                "- –ò—Å–ø–æ–ª—å–∑—É–π –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Å—Å—ã–ª–∫–∏, –Ω–∞–∑–≤–∞–Ω–∏—è –∏ —Ñ–∞–∫—Ç—ã –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤. \n"
                "–ï—Å–ª–∏ —Ç—ã –∏—Å–ø–æ–ª—å–∑—É–µ—à—å fetch_url: - —Ç—ã –æ–±—è–∑–∞–Ω –æ–ø–∏—Ä–∞—Ç—å—Å—è –Ω–∞ –ø–æ–ª—É—á–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç- –µ—Å–ª–∏ –∫–æ–Ω—Ç–µ–Ω—Ç –ø—É—Å—Ç –∏–ª–∏ –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–µ–Ω ‚Äî —Å–∫–∞–∂–∏ –æ–± —ç—Ç–æ–º –ø—Ä—è–º–æ- –∑–∞–ø—Ä–µ—â–µ–Ω–æ –≥–æ–≤–æ—Ä–∏—Ç—å, —á—Ç–æ —É —Ç–µ–±—è –Ω–µ—Ç –¥–æ—Å—Ç—É–ø–∞ –∫ —Å—Å—ã–ª–∫–µ"
            )

            print("[FC] Enforcing web_search usage policy")

            messages.append({
                "role": "system",
                "content": tools_policy
            })

    # 3Ô∏è‚É£ –§–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏ —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ tool
    final = client.chat.completions.create(
        model=model,
        messages=messages
    )

    print("[FC] Final answer generated")

    return final.choices[0].message.content

def log_web_search(query: str, results: list):
    print("\n" + "‚îÄ" * 18 + " WEB SEARCH " + "‚îÄ" * 18)
    print(f"Query: {query}")
    print(f"Results: {len(results)}\n")

    for i, r in enumerate(results, 1):
        title = (r.get("title") or "").strip()
        link = (r.get("link") or "").replace("https://", "").replace("http://", "")
        print(f"{i}. {title}")
        print(f"   üîó {link}")

    print("‚îÄ" * 54 + "\n")
